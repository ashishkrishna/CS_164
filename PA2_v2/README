Write-up for PA2
----------------
There are a few things to note in our implementation. First, handling for '<=' as a LE token was added, as this was missing. In addition, handling for backlashes in the YYINITIAL state was added as an error. This is in agreement with the behavior of the given lexer.

Our strings were implemented in a simple way. The first mach the straing state looks for is a quotation mark. This was, it can correctly start and stop reading strings. Following this, much of the handling is to match anything except escaped characters. This is added as text. Newline characters that are not escaped are made to throw errors, while escaped newlines as well as new line literals are handling in the obvious way. Null terminators and overly long strings are handled by simply checking at the end of the string if the error occured. This way, we were able to handle not only the issue of unescaped newlines in the proper way, but also to avoid having to search for the second quote character. By simple reading the string as a string until the end, and assessing the errors that were encountered, things go a lot more seamlessly.

Comments and single line comments each have their own state. In effect, they simply ignore all characters, with the [^] expression, except newline characters. Thus the regex was very simple to write! When a newline was found, the line count is increased, otherwise the input is ignored. From YYINITIAL the lexer looks for "*)" as well as "(*", in the first case to throw the unmatched error. Because nested comments are allowed, and matched parenthesis is not a regular language, a simple counter had to be implemented in the hard-coded portion of the lexer, to track how many levels into the comments the lexer is. 

An interesting note is how we implemented the white space and newline handler. A trivial implementation of this is to increase the count when just one \n is seen, and do nothing when \s is seen, in that priority. However, this takes two states. We implemented this in one state in our lexer, simply by checking each whitespace character in any block of white space, and counting the number of newlines in it, and updating the count accordingly. Whether or not having less states is helpful is another question.

As a last comment, we would like to talk about the test cases we chose. Much of the given tests test the Cool language, and using the .cl programs from PA1, we also were able to verifiy that our lexer gave the same output as the given lexer.py on these files. Naturally however, it is best for us to write our own tests, and moreover test incorrect code, and so this is in our test.cl file along with a snippet of the arith.cl code to serve as a correct Cool piece of code. Our testing tested all the edge cases of broken code, such as how the backslash worked with characters in strings, ending the file in an unfinished comment, nesting comments and single line comments, nesting strings in comments and vice versa, etc. In effect, our tests served as an exemplary sample of working cool code, and our best attempts at breaking our lexer by looking at all of the edge cases in the Cool language.
